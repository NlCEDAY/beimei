##                                                                                                        项目：



### K-MEANS概念：

- K：要得到的簇的个数（分组个数）

- 质心：均值，即向量各维取平均，迭代需要

- 距离度量：用欧氏距离和余弦相似度（需要先标准化参数）

- 优化目标：![image-20200825015519383](C:%5CUsers%5Clenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200825015519383.png)

  1-k一共有多少簇

  ci中心点 

  对于每个簇来说，它里的值到中心点的距离之和是越小越好



### 工作流程：

1. 随机选择两个分类的质心先算未知点到两个分类点的质心的距离，离哪个点近就是哪个类
2. 更新计算对分过类的两类群体，继续更新质心，再对每个点到两个质心的距离，进行再分类。
3. 不断迭代，直至分类清楚。

### 一：优化运行时间：

#### 1.增加并行度

> 确保使用整个集群的资源，不要把任务集中在某几个节点上，
>
> 对于shuffle的操作，**增加其并行度**来充分使用资源

#### 2.减少数据序列化，反序列化的负担

> SparkStreaming默认将接收到的数据 **序列化后**在存储，来减少内存使用
>
> 为了减少cpu负担，因此我们使用**Kryo序列化方式** 或者 **自定义序列化接口**

#### 3.设置合理的batch duration（批处理时间）

> SparkStreaming中，Job之间可能存在依赖关系，job必须 按照顺序递进执行，若上游的job超过了批处理时间，那么下游的job也会无法按时提交，然后不断往后job拖延，因此一定要设置合理，作业一定能在该时间间隔内完成

#### 4.减少因任务提交和分发所带来的负担

> Akka框架可以高效保证任务及时分发
>
> 担当批处理间隔非常小<500ms,提交和分发任务的延迟就显得不可忽视了
>
> 使用 Standalone和Coarse-grained Mesos模式通常会比使用Fine-grained Mesos模式有更小的延迟

### 二：优化内存使用：

#### 1.控制batch size （批处理时间间隔内的数据量）

> Spark Streaming 会把批处理时间间隔收到的数据全部放在Spark内的内存里，因此必须确保当前节点可用内存能容奶间隔内的所有数据。

#### 2.及时清理不再使用的数据

> Spark Streaming 会将接受的数据全部存储到内部可用内存区域，但不会说我这个批处理处理过就清理掉
>
> 所以对于不需要的数据即使清理，通过设置合理的spark.cleaner.ttl时长来即使清理超时的无用数据。

#### 3.观察及时调整GC策略

> GC会影响Job的正常运行，可能延长Job的执行时间，引起一系列不可预料的问题。 
>
> 采用不同的GC策略减小内存回收对Job运行的影响。

### 缓慢变化维：

> 面对的是时间的流逝，维度的属性会发生缓慢的变化

面对**是否保存历史数据变化**，我们提供了三种解决方法：

1. `重写维度值--不保留历史数据`

   > 直接overwrite

2. `插入新的维度行`

   > 做增量，采用历史拉链存储

   **用法** ：维护了一个历史数据表和一个更新表，使用join操作关联

   **优点**：占用磁盘适中，可以查看最新数据也可以查看历史数据

   **缺点**：效率相对较低

   ![image-20200914085637818](C:%5CUsers%5Clenovo%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200914085637818.png) 

   **第一步**：每天更新消息将更新表所有数据插入到拉链表中，将起始日期设置为今天，截至日期设置为9999-12-31

   将历史表进行leftjoin更新表，如果能够join上，那么就将历史表的截至日期改为昨天，使用overwrite覆盖掉。

   ```sql
   create table if not exists user_his(
   userid string,
   username string,
   userphone string,
   start_date string,
   end_date string,
   )
   stored as textfile
   
   create table if not exists user_update(
   userid string,
   username string,
   userphone string
   )
   stored as textfile
   
   
   insert into table user_his values
   ('1001','zhangsan','22222222','2020-09-01','9999-12-31'),
   ('1002','lisi','2222we22','2020-09-01','9999-12-31'),
   ('1003','wangwu','22ss2222','2020-09-01','9999-12-31')
   
   
   
   insert into table user_update values
   ('1001','zhangsan','111111111')
   ('1002','lisi','1wwww11111')
   
   
   ```

   **第二步**：每天更新user_his,一天更新多次，一天只算最后一条信息

   更细粒度可以把天改成小时

   ```sql
   insert overwrite table user_his
   select * from
   (
   select *
   '2020-09-14' start_date,
   '9999-12-31' end_date
   from user_update
   
   union all
   
   select
   uh.userid,
   uh.username,
   uh.userphone,
   uh.start_date,
   case when uu.userid is not null and uh.end_date = '9999-12-31' then '2020-09-13' else uh.end_date end end_date
   from user_his uh left join user_update uu on uu.userid = uh.userid
   ) t;
   ```


### #######具体项目

谁，什么，何处，何时，为何，如何

xxxx好

我叫xxx，来自江苏常州，毕业于南京工程学院，自己呢曾经在华威模具有限公司从事数据开发已有两年工作经验了，自己平常主要呢负责一些搭建数仓，ETL清洗呀，特征工程，机器学习预测等等相关的工作，工作之余啊，，会在自己的博客和公众号上呢写写文章，还会对自己的工作内容进行一个复盘，对咋们数据开发的相关技术栈再深入的研究从而发现一些模型建立上的漏洞，sql的优化啊，并对项目的架构进行完善。自己的i性格方面呢平时呢比较内敛沉稳，但是在与工作和朋友中呢我也是非常的放得开，并且自己也是一个很有毅力的人，每年也会去参加马拉松比赛，杭州作为大数据发展全国最好的地方，我个人呢非常看好杭州，我也了解过贵公司的一些基本信息，贵公司的岗位要求与我呢是非常符合，所以非常期待加入贵公司。

我们公司自研搭建这套智慧生产管理平台的**目的**是为了更好的提高设备的生产效率，降低产品的废品率，提高人员的绩效，和完成一个APS计划排程，合理规划库存和物流，对设备进行故障预测，减少非计划停机等等等等，**基于这些目的**，我们需要通过大数据的技术对企业现有的ERP,PDM,MES等系统进行集成数据，搭建数据仓库，对多个流程生成主题，完成多个数据源的统一分析和预测，摒弃了以往的人工报表的形式，给企业提供一个更好的决策分析系统。

我现在主要的项目呢是负责，生产管理和仓库管理这块的数据分析

我们的生产主题呢，主要是基于生产过程中的四大要素，人，料，法，机，来进行协调管理，通常呢，我们通过记录人员的计划工时，实际工时，最短最长工时，直行率，不良率，抽检退回数，抽检通过数，是否手动更改过工序，等来做绩效指标等指标对人员生产管理，

（SparkStreaming）对设备实时生产的数据比如启停时间啊，送料率，切割长度，深度，刀具的进给倍率，刀温，转动角度，x轴，y轴，z轴方向极大值，主轴负载，主轴转速，主轴方向，切削力，当前工艺，加工时间，来检测出设备运行状态，运作效率等指标对设备进行管理，比如设备状态监控啊，设备故障预测啊，设备生产效率，直行率，稼动率啊等等

对产品原料的原料的订购清单的订购数，库存主件损坏单的加工失败件数，丢失单的制品备件的丢失数，（配件）在制品库存单的已加工件数，加工单的计划加工数，失控订单的未完成数，机床生产效率的实际加工数，返工单的返工件数等指标来生产投入产出报表，与实际投入产出作差额，计算排队时间，拖欠量，进行生产物料管理，比如生产日程安排啊，生产批次跟踪，预计交期，APS啊，保持一个安全的库存量

还有对物流单的供应商种类，运输时间，运输方式，运输路径，运车大小，物品丢失单的丢失物品等的**物流成本**，库存单的损坏单制品损坏数，制品存放时间，工件存放位置，丢失单丢失数的**仓储成本**，加工时刀具的损坏，加工时间的能耗量，返工件数，加工人员，加工工时，废料损耗的**加工成本，采购成**本等等等生成 成本损失模型，利用随机森林来估算成本



对于加工产品的最终结果的好坏，对刀具进给速率，深度，转速，切削力，刀具规格，加工零件的工艺参数的各种组合变化，工序螺纹位置，开槽深度，面铣精度，钻孔位置工序组合的对**NC程序的优化以及加工顺序的优化，**



对产品信息追溯，对产品的原料信息，设备信息，工序加工检验时间，检验记录，审理记录，加工人员签名，出入库管理单等统计进行质量管理

材质信息，人员信息，环境

对于关重件信息，制成品损坏原因，和对应制成品种类，进行聚类分析，来对制成品质量预测啊等等

对其整个生产过程科学化，透明化，高效化



业务上的问题：

错误值，空值，由于工作人员人工填单的时候出现输入错误，很多采购人员不填采购表单，财务人员第三方供应商的具体名称不填，导致表格有空值，检验人员质检单的很多注塑指标规格，材料

> 能否根据产品号码追溯这批产品的所有生产过程信息？
>
> 统一条生产线需要混合组装多种型号产品的时候，能否自动校验和操作提示以防止不间装配错误，产品生产流程错误，产品混装和货品交接错误？
>
> 过去12小时内生产线出现最多的5种产品缺陷是什么？次品数量各是多少？能否及时纠正？
>
> 目前仓库以及前工序，中工序，后工序线上每种产品数量个是多少？
>
> 生产线和加工设备有多少时间在生产，多少时间在停转和空转。
>
> 能否废除人工报表，自动统计每个过程的生产数量，合格率和缺陷代码？ 
>
> 解决可以生产什么？
>
> 在什么时间生产什么？
>
> 在什么时间已经生产什么？
>
> 质量如何？
>
> 效益如何？
>
> 有多少数量可以交付？
>
> 生产计划完成状况如何？
>
> 生产班组，生产车间日/月绩效如何？
>
> 产量，质量，消耗，工艺操作水平如何？发生了什么问题？问题的原因是什么？
>
> 设备的运作效率如何？
>
> 工艺参数控制与目标值偏差如何？
>
> 目标值或额定值如何建立？
>
> 偏差的可能原因是哪些？
>
> 关键或更多的原因是什么？
>
> 过程有什么样的趋势？
>
> 统计的结果是什么？
>
> 过程现状如何？
>
> 过程产能是否可以提高？
>
> 过程消耗能否降低？
>
> 过程质量是否可以提高？
>
> 改进方向或重点在哪里？

### 避免Hive进行MapReduce

- **select * from xxx**
- where中过滤条件仅为**分区字段**时

> 执行下面命令，可以让Hive尝试使用本地模式执行操作：

```shell
set hive.exec.mode.local.auto=true;
```

### 二： JOIN优化：

- 多表join时，如果on子句连接键相同，那么将最大的表放在最后，Hive会尝试将**其他表缓存起来**，扫描最后 那个表进行计算（小表最大25M）

### 三：设置合理的reduce数：

MapReduce 程序中，reducer 个数的设定极大影响执行效率，这使得 Hive 怎样决定 reducer 个数成为一个关键问题。遗憾的是 Hive 的估计机制很弱，不指定 reducer 个数的情况下，Hive 会猜测确定一个 reducer 个数，基于以下两个设定：

- hive.exec.reducers.bytes.per.reducer（默认为 256000000
- hive.exec.reducers.max（默认为 1009）
- mapreduce.job.reduces=-1（设置一个常量 reducetask 数量）

计算 reducer 数的公式很简单： N=min(参数 2，总输入数据量/参数 1) 通常情况下，有必要手动指定 reducer 个数。考虑到 map 阶段的输出数据量通常会比输入有 大幅减少，因此即使不设定 reducer 个数，重设参数 2 还是必要的。

依据 Hadoop 的经验，可以将参数 2 设定为 0.95*(集群中 datanode 个数)。

### 四：优化Hive数据倾斜问题：

> 数据倾斜产生原因：
>
> - key分布不均匀。
> - map端数据倾斜，输入文件太多且大小不一 。
> - reduce端数据倾斜，分区器问题。
> - 业务数据本身的特征。



#### 业务场景解决：

---

> 空值数据过多，全部移到一个reduce端处理

`解决办法:给空值变成字符串+随机数`

---

> 不同数据类型关键字段关联，未处理的类型全部都分到一个reducer中

`解决办法:将数据类型转成一致`

---

> key值过于集中，很多key值分到一个reduce中

`解决办法:将key值集中的数据新生成一张小表存入内存，再使用mapjoin，在map端完成reduce`

---

#### 调节hive配置参数:

- **设置hive.map.aggr=true** 

  ---map端部分聚合，相当于Combiner

- **设置hive.groupby.skewindata=true**

  ---有数据倾斜时，查询计划生成两个mr job， 第一个job先进行key随机分配处理，先缩小数据量。第二个job再进行真正的group by key处理

----

### 五：JVM重用：

> 解决小文件和task特别多的场景

修改hadoop的mapred-site.xml文件进行设置

```xml
<property>
    <name>mapred.job.reuse.jvm.num.tasks</name>
    <value>10</value>
</property>

```

> 使得jvm实例在同一个job中重新使用N次，减少JVM启动造成的开销。

请简单自我介绍一下？

#### Hive执行顺序：

(1)from
(2)on
(3)join
(4)where
(5)group by
(6)having
(7)select
(8)distinct
(9)distribute by /cluster by
(10)sort by
(11) order by
(12) limit
(13) union /union all