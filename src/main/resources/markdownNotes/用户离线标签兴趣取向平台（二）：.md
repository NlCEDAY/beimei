## 用户离线标签兴趣取向平台（二）：

### 三：数据传输

#### Flume

> A **distributed**, **reliable**, and **available** service 

**◆ Stream Data** 

​	◼ Ingest streaming data from multiple sources into Hadoop for storage and analysis 

**◆ Insulate Systems**

​	◼ Buffer storage platform from transient spikes, when the rate of incoming data exceeds the rate at which data can be written to the destination 

> 它自己也可以缓冲数据channel

**◆ Guarantee Data Delivery** 

​	◼ Uses channel-based transactions to guarantee reliable message delivery. 

**◆ Scale horizontally** 

​	◼ Ingest new data streams and additional volume as needed.



### 2. Create the following topics in Kafka 

```shell
1. Start Apache Kafka if it is not started. ➢ Start Apache Kafka in Ambari; or ➢ In command line: o kafka start 
o kafka-server-start.sh ./config/server.properties 
 
2. Create the following topics in Kafka 
# Users 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic users --partitions 2 --replication-factor 1 

# User_Friends 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic user_friends_raw --partitions 2 --replication-factor 1 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic user_friends --partitions 2 --replication-factor 1

# Events 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic events --partitions 3 --replication-factor 1 

# Event_Attendees 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic event_attendees_raw --partitions 7 --replication-factor 1 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic event_attendees --partitions 2 --replication-factor 1 

# Train 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic train --partitions 2 --replication-factor 1 

#Test 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --create --topic test --partitions 2 --replication-factor 1 
 
#Delete a Topic 
#o In ambari, go to Kafka configuration page, set “delete topic enable” = true (under “Advanced kafka-broker” tab) 
#o Save & Restart Kafka 
 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --delete --topic topic-name 

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic users

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic user_friends_raw

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic user_friends 

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic users

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic event_attendees

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic event_attendees_raw

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic test

kafka-topics.sh --zookeeper 192.168.56.101:2181 --delete --topic train

#List all topics: 
kafka-topics.sh --zookeeper sandbox.hortonworks.com:2181 --list 
```

##### 一：创建文件夹

```shell
mkdir -p /var/flume/checkpoint/users
mkdir -p /var/flume/data/users
mkdir -p /events/input/intra/users
chmod -R 777 /events/input/intra
chmod -R 777 /var/flume

# user_friend
mkdir -p /var/flume/checkpoint/user_friends
mkdir -p /var/flume/data/user_friends
chmod 777 -R /var/flume

mkdir -p /events/input/intra/user_friends
chmod -R 777 /events/input/intra

# events
mkdir -p /var/flume/checkpoint/events
mkdir -p /var/flume/data/events
chmod 777 -R /var/flume

mkdir -p /events/input/intra/events
chmod -R 777 /events/input/intra

# event_attendees
mkdir -p /var/flume/checkpoint/event_attendees
mkdir -p /var/flume/data/event_attendees
chmod 777 -R /var/flume

mkdir -p /events/input/intra/event_attendees
chmod -R 777 /events/input/intra

# te
mkdir -p /var/flume/checkpoint/test
mkdir -p /var/flume/data/test
chmod 777 -R /var/flume

mkdir -p /events/input/intra/test
chmod -R 777 /events/input/intra


mkdir -p /var/flume/checkpoint/train
mkdir -p /var/flume/data/train
chmod 777 -R /var/flume

mkdir -p /events/input/intra/train
chmod -R 777 /events/input/intra


```



##### 二：flume文件配置传至阿米巴得配置

```shell

users.sources = usersSource
users.channels = usersChannel
users.sinks = usersSink

# Use a channel which buffers events in a directory
users.channels.usersChannel.type = file
users.channels.usersChannel.checkpointDir = /var/flume/checkpoint/users
users.channels.usersChannel.dataDirs = /var/flume/data/users

# Setting the source to spool directory where the file exists
users.sources.usersSource.type = spooldir
users.sources.usersSource.deserializer = LINE
# 当kafka数据记录大于flume全是数据，说明最大行数太小了
users.sources.usersSource.deserializer.maxLineLength = 6400
users.sources.usersSource.spoolDir = /events/input/intra/users
users.sources.usersSource.includePattern = users.*
users.sources.usersSource.interceptors = head_filter
users.sources.usersSource.interceptors.head_filter.type = regex_filter
users.sources.usersSource.interceptors.head_filter.regex = ^user_id,locale,birthyear,gender,joinedAt,location,timezone$
users.sources.usersSource.interceptors.head_filter.excludeEvents = true
users.sources.usersSource.channels = usersChannel

# Define / Configure sink
users.sinks.usersSink.type = org.apache.flume.sink.kafka.KafkaSink
users.sinks.usersSink.batchSize = 640
users.sinks.usersSink.brokerList = sandbox-hdp.hortonworks.com:6667
users.sinks.usersSink.topic = users
users.sinks.usersSink.channel = usersChannel




user_friends.sources = userFriendsSource
user_friends.channels = userFriendsChannel
user_friends.sinks = userFriendsSink

# Use a channel which buffers events in a directory
user_friends.channels.userFriendsChannel.type = file
user_friends.channels.userFriendsChannel.checkpointDir = /var/flume/checkpoint/user_friends
user_friends.channels.userFriendsChannel.dataDirs = /var/flume/data/user_friends

# Setting the source to spool directory where the file exists
user_friends.sources.userFriendsSource.type = spooldir
user_friends.sources.userFriendsSource.deserializer = LINE
user_friends.sources.userFriendsSource.deserializer.maxLineLength = 128000
user_friends.sources.userFriendsSource.spoolDir = /events/input/intra/user_friends
user_friends.sources.userFriendsSource.includePattern = userFriends.*
user_friends.sources.userFriendsSource.interceptors = head_filter
user_friends.sources.userFriendsSource.interceptors.head_filter.type = regex_filter
user_friends.sources.userFriendsSource.interceptors.head_filter.regex = user,friends
user_friends.sources.userFriendsSource.interceptors.head_filter.excludeEvents = true

user_friends.sources.userFriendsSource.channels = userFriendsChannel

# Define / Configure sink
user_friends.sinks.userFriendsSink.type = org.apache.flume.sink.kafka.KafkaSink
user_friends.sinks.userFriendsSink.batchSize = 640
user_friends.sinks.userFriendsSink.brokerList = sandbox-hdp.hortonworks.com:6667
user_friends.sinks.userFriendsSink.topic = user_friends_raw
user_friends.sinks.userFriendsSink.channel = userFriendsChannel





# Initialize agent's source, channel and sink
train.sources = trainSource
train.channels = trainChannel driverChannel
train.sinks = trainSink driverSink

# Use a channel which buffers events in a directory
train.channels.trainChannel.type = file
train.channels.trainChannel.checkpointDir = /var/flume/checkpoint/train
train.channels.trainChannel.dataDirs = /var/flume/data/train

# Setting the channel to memory
train.channels.driverChannel.type = memory
train.channels.driverChannel.capacity = 64000
train.channels.driverChannel.transactioncapacity = 16000

# Setting the source to spool directory where the file exists
train.sources.trainSource.type = spooldir
train.sources.trainSource.deserializer = LINE
train.sources.trainSource.deserializer.maxLineLength = 3200
train.sources.trainSource.spoolDir = /events/input/intra/train
train.sources.trainSource.includePattern = train.*
train.sources.trainSource.interceptors = head_filter i2
train.sources.trainSource.interceptors.head_filter.type = regex_filter
train.sources.trainSource.interceptors.head_filter.regex = user,event.*
train.sources.trainSource.interceptors.head_filter.excludeEvents = true
train.sources.trainSource.interceptors.i2.type = regex_extractor
train.sources.trainSource.interceptors.i2.regex = ^(\\d+)
train.sources.trainSource.interceptors.i2.serializers = s
train.sources.trainSource.interceptors.i2.serializers.s.name = key


train.sources.trainSource.channels = trainChannel driverChannel

# Define / Configure sink
train.sinks.trainSink.type = org.apache.flume.sink.kafka.KafkaSink
train.sinks.trainSink.batchSize = 640
train.sinks.trainSink.brokerList = sandbox-hdp.hortonworks.com:6667
train.sinks.trainSink.topic = train
train.sinks.trainSink.channel = trainChannel

# Setting the sink to HDFS
train.sinks.driverSink.type = hdfs
train.sinks.driverSink.hdfs.fileType = DataStream
train.sinks.driverSink.hdfs.filePrefix = train
train.sinks.driverSink.hdfs.fileSuffix = .csv
train.sinks.driverSink.hdfs.path = /user/events/driver/%Y-%m-%d
train.sinks.driverSink.hdfs.useLocalTimeStamp = true
train.sinks.driverSink.hdfs.batchSize = 6400
# Number of events written to file before it rolled (0 = never roll based on number of events)
train.sinks.driverSink.hdfs.rollCount = 3200
# File size to trigger roll, in bytes (0: never roll based on file size)
train.sinks.driverSink.hdfs.rollSize = 640000
# Number of seconds to wait before rolling current file (0 = never roll based on time interval)
train.sinks.driverSink.hdfs.rollInterval = 300
train.sinks.driverSink.channel = driverChannel




test.sources = testSource
test.channels = testChannel
test.sinks = testSink

# Use a channel which buffers events in a directory
test.channels.testChannel.type = file
test.channels.testChannel.checkpointDir = /var/flume/checkpoint/test
test.channels.testChannel.dataDirs = /var/flume/data/test

# Setting the source to spool directory where the file exists
test.sources.testSource.type = spooldir
test.sources.testSource.deserializer = LINE
test.sources.testSource.deserializer.maxLineLength = 6400
test.sources.testSource.spoolDir = /events/input/intra/test
test.sources.testSource.includePattern = test.*
test.sources.testSource.interceptors = head_filter
test.sources.testSource.interceptors.head_filter.type = regex_filter
test.sources.testSource.interceptors.head_filter.regex = user,event,invited.*
test.sources.testSource.interceptors.head_filter.excludeEvents = true

test.sources.testSource.channels = testChannel

# Define / Configure sink
test.sinks.testSink.type = org.apache.flume.sink.kafka.KafkaSink
test.sinks.testSink.batchSize = 640
test.sinks.testSink.brokerList = sandbox-hdp.hortonworks.com:6667
test.sinks.testSink.topic = test
test.sinks.testSink.channel = testChannel




# **********************************************************************************
# Deploy the following content into Flume
# -------------------------------------------------
# Initialize agent's source, channel and sink
event_attendees.sources = eventAttendeesSource
event_attendees.channels = eventAttendeesChannel
event_attendees.sinks = eventAttendeesSink

# Use a channel which buffers events in a directory
event_attendees.channels.eventAttendeesChannel.type = file
event_attendees.channels.eventAttendeesChannel.checkpointDir = /var/flume/checkpoint/event_attendees
event_attendees.channels.eventAttendeesChannel.dataDirs = /var/flume/data/event_attendees

# Setting the source to spool directory where the file exists
event_attendees.sources.eventAttendeesSource.type = spooldir
event_attendees.sources.eventAttendeesSource.deserializer = LINE
event_attendees.sources.eventAttendeesSource.deserializer.maxLineLength = 128000
event_attendees.sources.eventAttendeesSource.spoolDir = /events/input/intra/event_attendees
event_attendees.sources.eventAttendeesSource.includePattern = eventAttendees.*

event_attendees.sources.eventAttendeesSource.interceptors = head_filter
event_attendees.sources.eventAttendeesSource.interceptors.head_filter.type = regex_filter
event_attendees.sources.eventAttendeesSource.interceptors.head_filter.regex = event,yes,maybe.*
event_attendees.sources.eventAttendeesSource.interceptors.head_filter.excludeEvents = true
；；
event_attendees.sources.eventAttendeesSource.channels = eventAttendeesChannel

# Define / Configure sink
event_attendees.sinks.eventAttendeesSink.type = org.apache.flume.sink.kafka.KafkaSink
event_attendees.sinks.eventAttendeesSink.batchSize = 640
event_attendees.sinks.eventAttendeesSink.brokerList = sandbox-hdp.hortonworks.com:6667
event_attendees.sinks.eventAttendeesSink.topic = event_attendees_raw
event_attendees.sinks.eventAttendeesSink.channel = eventAttendeesChannel



events.sources = eventsSource
events.channels = eventsChannel
events.sinks = eventsSink1 eventsSink2 eventsSink3
events.sinkgroups = grpEvents
events.sinkgroups.grpEvents.sinks = eventsSink1 eventsSink2 eventsSink3
events.sinkgroups.grpEvents.processor.type = load_balance
events.sinkgroups.grpEvents.processor.backoff = true
events.sinkgroups.grpEvents.processor.selector = round_robin

# Use a channel which buffers events in a directory
events.channels.eventsChannel.type = file
events.channels.eventsChannel.checkpointDir = /var/flume/checkpoint/events
events.channels.eventsChannel.dataDirs = /var/flume/data/events
events.channels.eventsChannel.transactionCapacity = 5000

# Setting the source to spool directory where the file exists
events.sources.eventsSource.type = spooldir
events.sources.eventsSource.deserializer = LINE
events.sources.eventsSource.deserializer.maxLineLength = 32000
events.sources.eventsSource.spoolDir = /events/input/intra/events
events.sources.eventsSource.includePattern = ^events_[0-9]{4}-[0-9]{2}-[0-9]{2}.csv$
events.sources.eventsSource.interceptors = i
events.sources.eventsSource.interceptors.i.type = regex_filter
events.sources.eventsSource.interceptors.i.regex = event_id,user_id,start_time,city.*
events.sources.eventsSource.interceptors.i.excludeEvents = true
events.sources.eventsSource.channels = eventsChannel

# Define / Configure sinks
events.sinks.eventsSink1.type = org.apache.flume.sink.kafka.KafkaSink
events.sinks.eventsSink1.batchSize = 1280
events.sinks.eventsSink1.brokerList = sandbox-hdp.hortonworks.com:6667
events.sinks.eventsSink1.topic = events
events.sinks.eventsSink1.channel = eventsChannel
events.sinks.eventsSink2.type = org.apache.flume.sink.kafka.KafkaSink
events.sinks.eventsSink2.batchSize = 1280
events.sinks.eventsSink2.brokerList = sandbox-hdp.hortonworks.com:6667
events.sinks.eventsSink2.topic = events
events.sinks.eventsSink2.channel = eventsChannel
events.sinks.eventsSink3.type = org.apache.flume.sink.kafka.KafkaSink
events.sinks.eventsSink3.batchSize = 1280
events.sinks.eventsSink3.brokerList = sandbox-hdp.hortonworks.com:6667
events.sinks.eventsSink3.topic = events
events.sinks.eventsSink3.channel = eventsChannel




```



##### 三：flume去对应文件夹收集数据

```shell
 
 
 
 # 启动flume
flume-ng agent -n event_attendees -c conf -f /opt/flumeconf/conf_1012_events_attendees.properties

flume-ng agent -n events -c conf -f /opt/flumeconf/conf_1012_events.properties

flume-ng agent -n test -c conf -f /opt/flumeconf/conf_1012_test.properties

flume-ng agent -n train -c conf -f /opt/flumeconf/conf_1012_train.properties

flume-ng agent -n users -c conf -f /opt/flumeconf/conf_1012_user.properties

flume-ng agent -n user_friends -c conf -f /opt/flumeconf/conf_1012_user_friends.properties




# 把文件复制到flume对应文件夹
install -m 777 users.csv /events/input/intra/users/users_2020-09-23.csv

install -m 777 user_friends.csv /events/input/intra/user_friends/userFriends_2020-09-23.csv

install -m 777 events.csv /events/input/intra/events/events_2020-09-23.csv

install -m 777 event_attendees.csv /events/input/intra/event_attendees/eventsAttendees_2020-09-23.csv

install -m 777 test.csv /events/input/intra/test/test_2020-09-23.csv

install -m 777 train.csv /events/input/intra/train/train_2020-09-23.csv
```



##### 四：查看分区数据

```shell
kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list sandbox-hdp.hortonworks.com:6667 --topic users -time -1 --offsets 1


kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list sandbox-hdp.hortonworks.com:6667 --topic user_friends_raw -time -1 --offsets 1

kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list sandbox-hdp.hortonworks.com:6667 --topic events -time -1 --offsets 1

kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list sandbox-hdp.hortonworks.com:6667 --topic event_attendees_raw -time -1 --offsets 1

kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list sandbox-hdp.hortonworks.com:6667 --topic test -time -1 --offsets 1

kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list sandbox-hdp.hortonworks.com:6667 --topic train -time -1 --offsets 1


kafka-console-consumer.sh --bootstrap-server sandbox-hdp.hortonworks.com:6667 --topic train --from-beginning --property print.key=true
```



##### 创建Hbase表：

```shell
create 'events_db:users', 'profile', 'region', 'registration' 

create 'events_db:user_friend', 'uf' 

create 'events_db:events', 'schedule', 'location','creator','remark' 

create 'events_db:event_attendee', 'euat'

create 'events_db:train', 'eu' 
```

##### 启动HBase进程：

```
start-hbase.sh
hbase shell
```



### 3. Create the following tables in HBase:

#### Create events_db namespace in HBase 

```shell

```

```shell

```

