## 随机森林--极限森林--梯度提升树：

### 一：集成算法Ensemble learning

> 再讲解三种分类器的前提，先得聊一聊集成算法：

> The goal of **ensemble methods** is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.

>目标**集成方法**将几个弱学习器的预测与给定的学习算法结合起来，以提高单个学习器的通用性和鲁棒性（健壮性）



#### 1）Bagging：训练多个学习器取平均

> 在原始训练集的随机子集上建立几个黑箱学习器的实例，然后将它们的个体预测集合起来，形成最终的预测

- 回归取平均，分类则投票表决，颇有三个诸葛亮顶个臭皮匠的感觉

$$
f(x)= \frac{1}{M}\sum_{m=1}^Mf_m(x)
$$

> bootstrap aggregation 并行训练一堆分类器

**典型为随机森林**：**Bagging+决策树=随机森林**

- 随机：数据采样随机，特征选择随机
- 森林：很多决策树并行放在一起





#### 2）Boosting：从弱学习器开始加强，通过加权来进行训练

> 串行训练一堆分类器
>
> 对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。
>
> 随着**迭代的进行**，之前越没有成功预测的例子学习器在当前迭代过程中会**更加关心它**。因此，每个后继的弱学习者都**不得不集中精力去训练前面的示例所遗漏的示例上**

- Boosting颇像一个不断反省自己哪里做的不好的完美主义者

$$
F_m(x)=F_{m-1}(x) + argmin_k\sum_{i=1}^nL(y_i,F_{m-1}(x_i)+h(x_i))\\--加入一颗树，比原来强
$$



**典型代表AdaBoost，XgBoost**：

**AdaBoost + 决策树 = 提升树**

**GradientBoost + 决策树 = 梯度提升树**



##### AdaBoost：

> 根据前一次的分类效果调整数据给予权重 ，xx分类器这次分类结果出错了，我们下次重新分类时，给xx分类器更大的权重拿更多的数据，看看还会不会错，最后根据分类器的准确性（谁错的少）来确定各自的准确性权重，再合体综合一下。



#### 3）Stacking：聚合多个分类或回归模型（可以分阶段来做）

> 把多个算法放在一起大杂烩堆叠

- **分阶段：**第一阶段得出各自结果，第二阶段再用前一阶段结果训练

> stacking的像极了一个急切渴望学习但又没有自己一套方法的入门者，它会去把所有流行的模型都去集成一遍，看看哪个最好，他选哪个

### 二：集成模式下的竞争：随机森林--极限森林--梯度提升树

#### 1）大名鼎鼎的随机森林：

> 多个决策树学习器的选举或者平均的集成学习器

##### 单个决策树随机了什么？

- 样本是随机的：抽取的部分样本
- 特征是随机的：抽取的部分特征
- 参数是随机的：我佛系我自豪
- 模型是随机的：ID3,C4.5我随机选

##### 怎么构建？

1. 从原始训练集**随机有放回采样N个样本，M个特征，进行K次采样**，得到K个随机模型。
2. 训练其K个模型，每次分裂时根据**信息增益/信息增益比/基尼指数选择最好的特征**进行分裂，无剪枝操作。
3. 生成的K棵树组成森林，回归取均值，分类则选举。



##### 随机森林得优势：

- 能够处理高纬度数据，特征多，并且不用做特征选择

- 在训练完后，它能够给出哪些feature比较重要

  当破坏掉某个特征时，错误率显著上升会给感受到特征得重要性，随机森林随机采样特征可以做到。

- 容易做成并行化方法，速度快

- 可以进行可视化展示，便于分析

#### 2）不极限的极限森林：

> 多个决策树学习器的选举或者平均的集成学习器,
>
> 与随机森林的随机方式稍有差别

##### 单个决策树随机了什么？

- 样本**我不随机**：抽取全部样本
- 特征是随机的：抽取的部分特征
- 参数是随机的：我佛系我自豪
- 模型是随机的：ID3,C4.5我随机选
- 分裂是随机的：相比random forest的贪婪，我佛系之后的效果可能更好

##### 极限树与随机森林的主要区别：

- RandomForest应用的是Bagging模型样本有随机，ExtraTree**使用的所有的样本**，只是特征是随机选取的，

- 因为**分裂是随机的**，所以在某种程度上比随机森林得到的结果**更加好**;

  

### 三：附录Scikit-learn的randomForest和ExtraTrees的参数说明：

#### RandomForestClassifier参数列表：

- **n_estimators--n估计量**： Int，默认值=100

  森林中树木的数量。*在0.22版中更改：*的默认值`n_estimators`0.22从10变到100。

- **criterion--标准：**{“基尼”、“熵”}，默认=“基尼”

  测量分割质量的功能。支持的准则是吉尼杂质的“基尼”和信息增益的“熵”。注意：这个参数是树特有的.

- **max_depth--最大深度：**INT，默认值=无

  树的最大深度。如果没有，那么节点会被展开，直到所有的叶子都是纯的，或者直到所有的叶子包含的样本少于min_SAMSACTS_Spl

- **min_samples_split--最小样本分裂：**整数或浮点数，默认值=2

  拆分内部节点所需的最小样本数：如果int，那么考虑`min_samples_split`作为最小数目。如果浮动，那么`min_samples_split`是一个分数`ceil(min_samples_split * n_samples)`是每次分割的最小样本数。*在0.18版中更改：*添加分数的浮点值。

- **min_samples_leaf--最小样本叶：**整数或浮点数，默认值=1

  一个叶节点所需的最小样本数。任何深度的分裂点只有在它离开的时候才会被考虑。`min_samples_leaf`每个左、右分支的训练样本。这可能会使模型平滑，特别是在回归过程中。如果int，那么考虑`min_samples_leaf`作为最小数目。如果浮动，那么`min_samples_leaf`是一个分数`ceil(min_samples_leaf * n_samples)`每个节点的最小样本数。*在0.18版中更改：*添加分数的浮点值。

- **min_weight_fraction_leaf--最小重量分数叶：**浮点数，默认值为0.0

  (所有输入样本的)权重之和的最小加权分数，需要在一个叶节点上。当不提供样品重量时，样品具有相同的重量。

- **max_features--最多选取特征数：**{“自动”、“sqrt”、“log 2”}、int或Float，默认值=“AUTO”

  在寻找最佳分割时要考虑的功能数量：如果int，那么考虑`max_features`每次分裂都有特征。如果浮动，那么`max_features`是一个分数`int(max_features * n_features)`特征在每次分割时都会考虑。如果“自动”，那么`max_features=sqrt(n_features)`.如果“sqrt”，那么`max_features=sqrt(n_features)`(与“自动”相同)。如果“log 2”，则`max_features=log2(n_features)`.如果没有，那么`max_features=n_features`.注意：除非找到节点样本的至少一个有效分区，否则对拆分的搜索不会停止，即使它需要有效地检查`max_features`特征。

- **max_leaf_nodes--最大叶节点：**INT，默认值=无

  种树`max_leaf_nodes`以最好的方式。最佳节点被定义为杂质的相对减少。如果没有，那么叶节的数量是无限的。

- **min_impurity_decrease--最小杂质减少：**浮点数，默认值为0.0

  如果这种拆分导致大于或等于此值的杂质减少，则节点将被拆分。加权杂质减少方程如下：`N_t / N * (impurity - N_t_R / N_t * right_impurity                    - N_t_L / N_t * left_impurity) `哪里`N`是样本的总数，`N_t`是当前节点上的样本数，`N_t_L`是左子中的样本数，以及`N_t_R`是正确的孩子中的样本数。`N`, `N_t`, `N_t_R`和`N_t_L`都引用加权和，如果`sample_weight`都通过了。*新版本0.19。*

- **min_impurity_split--最小杂质分裂：**浮动，默认=无

  树木生长早期停止的阈值。如果一个节点的杂质超过阈值，它就会分裂，否则它就是一片叶子。*自0.19版以来已不再推荐：*`min_impurity_split`已经被反对赞成`min_impurity_decrease`在0.19。的默认值`min_impurity_split`在0.23中从1e-7变为0，在0.25中将被移除。使用`min_impurity_decrease`相反。

- **bootstrap--引导：**Bool，默认值=True

  创建树时是否使用引导示例。如果为false，则使用整个数据集构建每个树。

- **oob_scoreOOB评分:**Bool，默认值=false

  是否使用包外样本来估计泛化精度.

- **n_jobs--并行度：**INT，默认值=无

  并行运行的作业数。[`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit), [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict), [`decision_path`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.decision_path)和[`apply`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.apply)都是平行的。`None`指1，除非在[`joblib.parallel_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend)背景。`-1`意味着使用所有处理器。看见[术语表](https://scikit-learn.org/stable/glossary.html#term-n-jobs)更多细节。

- **random_state--随机状态：**INT或RandomState，默认值=无

  控制在构建树时使用的样本引导的随机性(如果`bootstrap=True`)以及在每个节点上寻找最佳拆分时要考虑的特性的抽样(如果`max_features < n_features`)。看见[术语表](https://scikit-learn.org/stable/glossary.html#term-random-state)关于细节。

- **verbose--控制构建数过程的冗长度：**Int，默认值=0

  在拟合和预测时控制详细。

- **warm_start--暖启动：**Bool，默认值=false

  当设置为`True`，重用以前调用的解决方案来拟合，并在集合中添加更多的估计器，否则，只需适应一个全新的林。看见[词汇](https://scikit-learn.org/stable/glossary.html#term-warm-start).

- **class_weight--类别权重：**{“平衡”、“平衡子样本”}、dict或dicts列表，默认值=无

  与表单中的类关联的权重`{class_label: weight}`。如果不给，所有的课程都应该有一个重量。对于多输出问题，可以提供与y列相同的DECTS列表。注意，对于多输出(包括多标签)，应该为每个列的每个类定义其自己的DECT中的权重。例如，对于四类多标签分类权重应该是[{0：1，1：1}，{0：1，1：5}，{0：1，1：1}，{0：1，1：1}]，而不是[{1：1}，{2：5}，{3：1}，{4：1}]。“平衡”模式使用y的值自动调整权重，与输入数据中的类频率成反比，如下所示`n_samples / (n_classes * np.bincount(y))`“Balance_subSample”模式与“Balance”模式相同，只是根据每棵树的引导样本计算权重。对于多输出，y的每一列的权重将乘以.请注意，如果指定了SAMPLE_WART，这些权重将与SAMPLE_WART(通过FIT方法传递)相乘

- **ccp_alpha--CCPα：**非负浮点数，默认值为0.0

  复杂度参数用于最小成本-复杂度剪枝。具有最大成本复杂度的子树，该子树小于`ccp_alpha`将被选中。默认情况下，不执行剪枝。看见[最小成本-复杂度剪枝](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)关于细节。*新版本0.22。*

- **max_samples--最大样本数：**INT或Float，默认值=无

  如果引导为真，则从X中抽取样本数来训练每个基估计器。如果无(默认)，则绘制`X.shape[0]`样本。如果int，则绘制`max_samples`样本。如果浮动，然后画`max_samples * X.shape[0]`样本。因此，`max_samples`应该在间隔内`(0, 1)`.

#### ExtraForestClassifier参数列表：

> 参数列表与随机森林一致